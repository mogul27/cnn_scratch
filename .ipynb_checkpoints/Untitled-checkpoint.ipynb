{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional net for greyscale square images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 28, 28), y=(60000,)\n",
      "Test: X=(10000, 28, 28), y=(10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This bit will change in assignment depending on data coming in \n",
    "# load dataset\n",
    "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
    "# plot first few images\n",
    "for i in range(1):\n",
    "\t# define subplot\n",
    "\tplt.subplot(330 + 1 + i)\n",
    "\t# plot raw pixel data\n",
    "\tplt.imshow(trainX[i], cmap=plt.get_cmap('gray'))\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_feature_maps(num_filters_layer, filter_xy):\n",
    "    \n",
    "    assert filter_xy % 2 != 0, \"Filter dimensions must be an odd number\"\n",
    "    # Initialise according to standard normal distribution (Find paper)\n",
    "    # Initialising as all 0s would imeeditately set the multiplications to 0\n",
    "    layer_feature_maps = [np.random.standard_normal(size=(filter_xy, filter_xy)) for i in range(num_filters_layer)]\n",
    "    \n",
    "    return layer_feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(fil, layer_input):\n",
    "    \n",
    "    # Trace the position of the top left value on the filter\n",
    "    fil_x = 0\n",
    "    fil_y = 0\n",
    "    \n",
    "    # Use image anf filter dimensions to work out final_position of filter top left value \n",
    "    # Since both are square matrices, the value will be the same for x and y\n",
    "    max_index = layer_input.shape[0] - fil.shape[0] + 1 # +1 to accont for 0 indexing\n",
    "    print(max_index)\n",
    "    \n",
    "    while fil_y <= max_index:\n",
    "        # reset x to 0 to start a new row of convolution\n",
    "        fil_x = 0\n",
    "        \n",
    "        while fil_x <= max_index:\n",
    "            \n",
    "            # Perform the convolution operation returning the feature map as an array of products between filter \n",
    "            # and initial input\n",
    "            product_array = np.zeros(fil.shape[0], fil.shape[1])\n",
    "            \n",
    "            # Multiply all filter values with image values elementwise at current filter position\n",
    "            for x in fil.shape[0]:\n",
    "                for y in fil.shape[1]:\n",
    "                    # fil[0] gives 1 row of filter and fil[x][y] gives element y of row x\n",
    "                    # Same procdure but location of filter added for layer_input\n",
    "                    product_array[x][y] = fil[x][y] * layer_input[fil_x + x][fil_y + y]\n",
    "            \n",
    "            fil_x += 1\n",
    "        \n",
    "        fil_y += 1\n",
    "        \n",
    "    return product_array\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow popular architecture in https://arxiv.org/pdf/1511.08458.pdf\n",
    "# Start by following a single image through with a single filter\n",
    "\n",
    "\n",
    "\n",
    "def convolutional_layer(layer_input, layer_feature_maps, stride):\n",
    "    \"\"\"Convolve filter over input from image or previous layer\"\"\"\n",
    "    \n",
    "    # Assuming square input\n",
    "    filter_dimensions = layer_feature_maps[0].shape[0]\n",
    "    image_dimensions = layer_input.shape[0]\n",
    "    \n",
    "    # Calculate layers output dimensions according to formula described in \n",
    "    # https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8308186\n",
    "    output_size = 1 + (image_dimensions-filter_dimensions)/stride\n",
    "    \n",
    "    \n",
    "    output_feature_maps = []\n",
    "    \n",
    "    # Use fil since filter is a type keyword in Python\n",
    "    # Gives a 3x3 matrix of zeroes\n",
    "    for fil in layer_feature_maps:\n",
    "        output_feature_maps.append(convolve(fil, layer_input))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.51841035,  0.49914658, -0.06975927, -0.28804283,  1.80152882],\n",
      "       [ 0.33201144,  0.66870864,  0.13436851, -0.49325159, -0.28608298],\n",
      "       [-0.58702725, -0.01783633, -0.25682105,  2.10478478,  0.52025641],\n",
      "       [-1.61884281,  0.70920869,  1.07202814, -0.7030948 ,  1.82493444],\n",
      "       [ 0.31615226,  0.35524616,  2.16730611, -0.55252966, -0.86178718]])]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# No depth because using greyscale images\n",
    "layer_feature_maps = init_feature_maps(1, 5)\n",
    "feature_maps = convolutional_layer(trainX[0], layer_feature_maps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-add\n",
    "# Deptj\n",
    "# Zero padding\n",
    "# Initialisation of zero matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv55a6efa1d26343c8a8ba6f830d69db03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
